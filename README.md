# thmanyahâ€‘task

A selfâ€‘contained **realâ€‘time data pipeline** that turns raw userâ€‘engagement events into enriched analytics streams, all in Docker.

```
Kafka â†’ FlinkÂ SQLÂ (joins + metrics) â†’ Kafka
           â†‘                       â†“
     PostgreSQL (content dim)   (optional) Redis / BigQuery
```

* **PostgreSQLÂ 15**Â â€” content dimension table (`content`)
* **KafkaÂ 7.5.3**Â Â Â Â â€” raw (`engagements`) & processed (`processed_engagements`) event topics
* **FlinkÂ 1.17.1**Â Â Â â€” streaming job written in pure FlinkÂ SQL
* **DockerÂ Compose** â€” oneâ€‘liner spinâ€‘up on any laptop

---

## 1Â Â Architecture at a glance

| Layer                           | Purpose                             | Image / Tag                   |
| ------------------------------- | ----------------------------------- | ----------------------------- |
| **Postgres**                    | Lookup metadata (content catalogue) | `postgres:15`                 |
| **Kafka**                       | Source + sink event buses           | `confluentinc/cpâ€‘kafka:7.5.3` |
| **Flink**                       | Stateful streaming job              | `flink:1.17.1`                |
| **(Optional) Redis / BigQuery** | Serve dashboards & heavy BI         | *add later*                   |

---

## 2Â Â Where we are now Â ğŸš¦

| Milestone                              | Status                    | Notes                                                       |
| -------------------------------------- | ------------------------- | ----------------------------------------------------------- |
| Stack boots via `docker compose up -d` | âœ…                         | Kafka,Â Flink,Â Postgres all healthy                          |
| Postgres seeded (`Query.sql`)          | âœ…                         | `content` rows loaded                                       |
| Flink connectors jarred                | âœ…                         | Added Kafka & JDBC drivers under `flink-jars/`              |
| Flink tables created                   | âœ…                         | `engagement_events`, `content_dim`, `processed_engagements` |
| Main `INSERT` job submitted            | **ğŸŸ¡Â Failed / Iterating** | Typeâ€‘casting edgeâ€‘case (UUIDâSTRING) caused *FAILED* state  |
| Extra sinks (Redis / BigQuery)         | â©                         | Outâ€‘ofâ€‘scope for current sprint                             |

---

## 3Â Â Quickâ€‘start (copyâ€‘paste friendly)

### 3.0Â Â Prerequisites

```bash
# docker + compose
brew install docker
# local helpers
pip install psycopg2â€‘binary kafkaâ€‘python
```

### 3.1Â Â Clone & Launch

```bash
git clone <repoâ€‘url>
cd thmanyahâ€‘task
docker compose up -d    # ~30Â sec
```

### 3.2Â Â Add Flink connectors (oneâ€‘time)

```bash
mkdir -p flink-jars
# Kafka connector & client
wget -P flink-jars https://repo1.maven.org/maven2/org/apache/flink/flink-connector-kafka/1.17.1/flink-connector-kafka-1.17.1.jar
wget -P flink-jars https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.0/kafka-clients-3.4.0.jar
# JDBC connector & Postgres driver
wget -P flink-jars https://repo1.maven.org/maven2/org/apache/flink/flink-connector-jdbc/3.1.2-1.17/flink-connector-jdbc-3.1.2-1.17.jar
wget -P flink-jars https://jdbc.postgresql.org/download/postgresql-42.6.0.jar

docker compose down && docker compose up -d   # reload JARs
```

### 3.3Â Â Seed Postgres

```bash
# loads both `content` and an initial slice of `engagement_events`
docker exec -it thmanyah-task-postgres-1 psql -U thmanyah -d thmanyah_db -f /Query.sql
```

### 3.4Â Â Create Flink objects & start job

```bash
# open Flink SQLÂ CLI
docker compose exec flink-jobmanager ./bin/sql-client.sh

-- 1ï¸âƒ£Â KafkaÂ source\CREATE TABLE engagement_events (
  id BIGINT,
  content_id STRING,
  user_id STRING,
  event_type STRING,
  event_ts TIMESTAMP_LTZ(3),
  duration_ms INT,
  device STRING,
  raw_payload MAP<STRING,STRING>,
  WATERMARK FOR event_ts AS event_ts - INTERVAL '5' SECOND
) WITH (
  'connector'='kafka',
  'topic'='engagements',
  'properties.bootstrap.servers'='kafka:9092',
  'scan.startup.mode'='earliest-offset',
  'format'='json'
);

-- 2ï¸âƒ£Â JDBC lookup view (content)
CREATE TABLE content_dim (
  id STRING,
  slug STRING,
  title STRING,
  content_type STRING,
  length_seconds INT,
  publish_ts TIMESTAMP(3)
) WITH (
  'connector'='jdbc',
  'url'='jdbc:postgresql://postgres:5432/thmanyah_db',
  'table-name'='content',
  'username'='thmanyah',
  'password'='thmanyah123',
  'driver'='org.postgresql.Driver'
);

-- 3ï¸âƒ£Â Upsert sink for metrics
CREATE TABLE processed_engagements (
  content_id STRING,
  user_id STRING,
  event_type STRING,
  engagement_seconds DOUBLE,
  engagement_pct DOUBLE,
  PRIMARY KEY (content_id, user_id, event_ts) NOT ENFORCED
) WITH (
  'connector'='kafka',
  'topic'='processed_engagements',
  'properties.bootstrap.servers'='kafka:9092',
  'format'='json'
);

-- 4ï¸âƒ£Â ETL query (streaming)
INSERT INTO processed_engagements
SELECT
  e.content_id,
  e.user_id,
  e.event_type,
  e.duration_ms / 1000.0                      AS engagement_seconds,
  CASE
    WHEN c.length_seconds > 0 THEN ROUND((e.duration_ms / 1000.0) / c.length_seconds * 100, 2)
    ELSE NULL
  END AS engagement_pct
FROM engagement_events e
LEFT JOIN content_dim FOR SYSTEM_TIME AS OF e.event_ts AS c
ON e.content_id = c.id;
```

`INSERT` success prints a JobÂ IDâ€”verify on `http://localhost:8081`.

### 3.5Â Â EmitÂ / Verify a test event

---

## 4Â Â Troubleshooting Cheatsheet

| â˜ ï¸ Error                                   | Root cause                                 | How to unblock                                                                    |
| ------------------------------------------ | ------------------------------------------ | --------------------------------------------------------------------------------- |
| `relation "content" does not exist`        | DB seed missed                             | Repeat *3.3*                                                                      |
| `java.lang.ClassCastException UUIDâ†’String` | Flink JDBC converts UUID â†’ STRING manually | Add a `.cast(BINARY(16))` or store IDs as VARCHAR in Postgres                     |
| Job stuck in **FAILED** & UI empty         | Running in detached session                | `docker compose exec flink-jobmanager flink list -a` then `flink cancel <job-id>` |

---

## 5Â Â Next steps (TODO)

1. Fix UUID typeâ€‘mismatch & reâ€‘submit job
2. Add Redis sink for 5â€‘minute rolling topâ€‘N view
3. Wire BigQuery batch backfill path

---

## 6Â Â Contributing

PRs & issues welcomeÂ â€” this is a hiring takeâ€‘home, not production code, so any feedback is gold!

---

Â©Â 2025Â Thmanyah â€“Â Data Engineering Exercise
